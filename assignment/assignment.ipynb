{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55219fe9",
   "metadata": {},
   "source": [
    "# BeyondChats Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c08ff6",
   "metadata": {},
   "source": [
    "Your task is to develop a script that does the following: \n",
    "\n",
    "1. Takes as input a reddit user’s profile URL. \n",
    "\n",
    "Example:  \n",
    "\n",
    "https://www.reddit.com/user/kojied/ \n",
    "\n",
    "https://www.reddit.com/user/Hungry-Move-6603/ \n",
    "\n",
    "2. Scrapes comments and posts created by the redditor. \n",
    "\n",
    "3. Builds a User Persona based on details found on their reddit. \n",
    "\n",
    "Click this image link to understand what a typical user persona looks like. \n",
    "\n",
    "4. Output the user persona for the input profile in a text file. \n",
    "\n",
    "5. For each characteristic in the user persona, the script also “cites” the posts or \n",
    "comments it used to extract the specific user persona information. \n",
    "\n",
    "Technologies to Use: \n",
    "\n",
    "Feel free to use any tools or libraries or programming languages needed to accomplish this \n",
    "task. The use of LLMs is encouraged.\n",
    "\n",
    "(The final code is in last cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73613efc",
   "metadata": {},
   "source": [
    "first is to get reddit credentials of app and then scrape the data of specified user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fcc19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample submissions: [{'title': 'I feel violated by intern season', 'body': \"There's this bar that I frequent a few blocks away from my house. Technically different neighborhood, but generally has a mature vibe with people who like music and to dance. Today when I went to the bar and all of a sudden I felt like I was at a college basement party. I thought that I must have been hallucinating, but Then a girl pointed at me and referred to me as the old person. That's when I knew I had to get out of there.\\n I was at the wrong party. \\n\\nAs I walk back home, all of a sudden I realize that there were now tens of thousands of 18 year-old 19 year-old 20-year-olds who were all living on top of each other and were super young and horny to basically use the New York nightlife as their own personal orgy dome.\\n Naturally I felt violated\\n\\nBut then again, I thought to myself. Am I not the same as they are, just at a longer time horizon? I'm also a transient being in the city. They might only be here for three months, but I've only been here for three years as well. As a matter fact, there are bunch of people have done this for generations where their parents and their grandparents and their grandparents all met in New York, and they all decided to live somewhere in the broader region whether it be Westchester, Wilmington, Big Indian, Boston, etc. \\nNew York City is equally adventurous to each, so who am I to judge how others enjoy this adventurous city.\", 'url': 'https://www.reddit.com/r/newyorkcity/comments/1lykkqf/i_feel_violated_by_intern_season/', 'permalink': '/r/newyorkcity/comments/1lykkqf/i_feel_violated_by_intern_season/', 'created_utc': 1752383012.0, 'subreddit': 'newyorkcity'}, {'title': 'H1B holders, what are your thoughts on the narrative that you are being exploited?', 'body': '', 'url': 'https://www.reddit.com/r/AskReddit/comments/1hnx8j0/h1b_holders_what_are_your_thoughts_on_the/', 'permalink': '/r/AskReddit/comments/1hnx8j0/h1b_holders_what_are_your_thoughts_on_the/', 'created_utc': 1735358070.0, 'subreddit': 'AskReddit'}]\n",
      "Sample comments: [{'body': 'Sorry to hear that man, in the future you may want to have multiple granaries, one close to your berries/deer/fish and one for your veggies/apples/honey.\\nAlso I just store ale at the tavern or keep it in the brewery for this reason', 'permalink': '/r/ManorLords/comments/1ly3nuf/my_granary_burned_down/n2r16t2/', 'created_utc': 1752337758.0, 'subreddit': 'ManorLords'}, {'body': 'Yeah but it doesn’t seem to affect much though', 'permalink': '/r/ManorLords/comments/1lwfdh8/anyone_else_having_an_issue_where_their_trading/n2dn0vm/', 'created_utc': 1752161008.0, 'subreddit': 'ManorLords'}]\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "# Fill in your credentials here\n",
    "client_id = 'Ze0l8Tb5iILjJLTDxKRflA'\n",
    "client_secret = 'PcqO8CtAQO7pIoGuotuNeb_qPWrAMQ'\n",
    "user_agent = 'my_reddit_scraper'  # You can use any descriptive string\n",
    "\n",
    "# Initialize Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "def extract_username(url):\n",
    "    import re\n",
    "    match = re.search(r'reddit\\.com/user/([^/?]+)', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "profile_url = 'https://www.reddit.com/user/kojied/'\n",
    "username = extract_username(profile_url)\n",
    "\n",
    "# Fetch user submissions (posts)\n",
    "user = reddit.redditor(username)\n",
    "submissions = []\n",
    "for submission in user.submissions.new(limit=10):  # adjust limit as needed\n",
    "    submissions.append({\n",
    "        'title': submission.title,\n",
    "        'body': submission.selftext,\n",
    "        'url': submission.url,\n",
    "        'permalink': submission.permalink,\n",
    "        'created_utc': submission.created_utc,\n",
    "        'subreddit': str(submission.subreddit)\n",
    "    })\n",
    "\n",
    "# Fetch user comments\n",
    "comments = []\n",
    "for comment in user.comments.new(limit=10):  # adjust limit as needed\n",
    "    comments.append({\n",
    "        'body': comment.body,\n",
    "        'permalink': comment.permalink,\n",
    "        'created_utc': comment.created_utc,\n",
    "        'subreddit': str(comment.subreddit)\n",
    "    })\n",
    "\n",
    "print(\"Sample submissions:\", submissions[:2])\n",
    "print(\"Sample comments:\", comments[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4430f",
   "metadata": {},
   "source": [
    "This is a trail of loading a sentence transformer to embed the data scraped so that it can be stored in Faiss vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23bcdf",
   "metadata": {},
   "source": [
    "I am using \"all-MiniLM-L6-v2 \" sentence transformer which i already have in my local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72887023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence Transformer model from local path\n",
    "model = SentenceTransformer(r\"C:\\amrita_uni\\Projects\\BeyondChats\\model\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad28dc43",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863eabf",
   "metadata": {},
   "source": [
    "so combining all , first use praw to connect to reddit and scrape the user comments and posts.\n",
    "\n",
    "then have a list of persona fields so that RAG can help search up the data stored in Faiss for top posts or comments related to the persona field\n",
    "\n",
    "And finally we will be using ollama mistral which i already have it in my local , so this first has a basic common prompt mentioning what output i need , is coupled with the personal field and sent to the LLM as input . \n",
    "this works iteratively for each field and citation is also done since RAG feeds in the doc(which contains link ,body,type,etc ) to LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e2225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\velam\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Your Reddit API credentials\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"Ze0l8Tb5iILjJLTDxKRflA\",\n",
    "    \"client_secret\": \"PcqO8CtAQO7pIoGuotuNeb_qPWrAMQ\",\n",
    "    \"user_agent\": \"my_reddit_scraper\"\n",
    "}\n",
    "\n",
    "# Path to your local SentenceTransformer model folder\n",
    "EMBEDDING_MODEL_PATH = r\"C:\\amrita_uni\\Projects\\BeyondChats\\model\"\n",
    "\n",
    "# Persona fields to extract\n",
    "PERSONA_FIELDS = [\n",
    "    \"gender\", \"age\", \"build\", \"hair\", \"skin tone\", \"clothing\", \"setting\", \"posture\",\n",
    "    \"overall impression\", \"fashion sense\", \"lifestyle cues\", \"frustrations\", \"behaviors\",\n",
    "    \"habits\", \"goals and needs\", \"motivation\", \"personality\", \"persona quote\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_username(url):\n",
    "    match = re.search(r'reddit\\.com/user/([^/?]+)', url)\n",
    "    return match.group(1) if match else url\n",
    "\n",
    "def fetch_and_save_reddit_user_data(reddit, profile_url, save_path='reddit_user_data.json', limit=100):\n",
    "    username = extract_username(profile_url)\n",
    "    user = reddit.redditor(username)\n",
    "    data = []\n",
    "    for post in user.submissions.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'post',\n",
    "            'title': post.title,\n",
    "            'body': post.selftext,\n",
    "            'permalink': f'https://reddit.com{post.permalink}',\n",
    "            'subreddit': str(post.subreddit)\n",
    "        })\n",
    "    for comment in user.comments.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'comment',\n",
    "            'body': comment.body,\n",
    "            'permalink': f'https://reddit.com{comment.permalink}',\n",
    "            'subreddit': str(comment.subreddit)\n",
    "        })\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Saved {len(data)} posts/comments to {save_path}\")\n",
    "\n",
    "def load_reddit_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def build_faiss_index(docs, embedding_model_path):\n",
    "    model = SentenceTransformer(embedding_model_path)\n",
    "    texts = [\n",
    "        (f\"Title: {doc['title']}\\nBody: {doc['body']}\" if doc['type']=='post' else doc['body'])\n",
    "        for doc in docs\n",
    "    ]\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, docs, model\n",
    "\n",
    "def retrieve_docs(query, index, docs, model, k=7):\n",
    "    emb = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(emb, k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "def build_prompt_for_field(field, docs):\n",
    "    prompt = f\"Based only on these Reddit entries, describe the user's {field}.\\n\"\n",
    "    prompt += \"For every statement, cite with a snippet or the post/comment link. If info is missing, state 'Not enough data.'\\n\\n\"\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        txt = doc.get('title', '') + \" \" + doc.get('body', '') if doc['type']=='post' else doc.get('body', '')\n",
    "        txt = txt.strip().replace('\\n', ' ')\n",
    "        snippet = txt[:300]\n",
    "        prompt += f\"[{i}] {snippet} ... ({doc['permalink']})\\n\"\n",
    "    prompt += \"\\nYour answer:\"\n",
    "    return prompt\n",
    "\n",
    "def run_ollama_mistral(prompt):\n",
    "    result = subprocess.run(\n",
    "        ['ollama', 'run', 'mistral'],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding='utf-8'  # Explicitly use UTF-8\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def persona_pipeline(profile_url, persona_fields, reddit_credentials, embedding_model_path, docs_limit=70):\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_credentials['client_id'],\n",
    "        client_secret=reddit_credentials['client_secret'],\n",
    "        user_agent=reddit_credentials['user_agent']\n",
    "    )\n",
    "    fetch_and_save_reddit_user_data(reddit, profile_url, 'reddit_user_data.json', limit=docs_limit)\n",
    "    docs = load_reddit_data('reddit_user_data.json')\n",
    "    index, meta, embed_model = build_faiss_index(docs, embedding_model_path)\n",
    "    results = {}\n",
    "    for field in persona_fields:\n",
    "        context = retrieve_docs(field, index, meta, embed_model, k=7)\n",
    "        prompt = build_prompt_for_field(field, context)\n",
    "        ans = run_ollama_mistral(prompt)\n",
    "        results[field] = ans\n",
    "        print(f\"\\n==== {field.upper()} ====\\n{ans}\\n\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6012142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this URL to analyze a different user\n",
    "reddit_user_url = 'https://www.reddit.com/user/kojied/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244f7e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 101 posts/comments to reddit_user_data.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36e04f46165248abad66f89d28948583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== GENDER ====\n",
      " Based on the provided data, it is not possible to definitively determine the user's gender. The Reddit entries do not contain any gender-specific information or pronoun usage.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== AGE ====\n",
      " Based on the provided Reddit entries, it's challenging to pinpoint a specific age for the user. However, some observations can be made regarding the user's familiarity with different generations (Gen-Z and Millennials), interest in retro games, and anticipation of future technology which may suggest a younger demographic. Additionally, the user mentions the year 2034, indicating they are alive after that year, but this does not provide information about their current age.\n",
      "\n",
      "Here is an excerpt for the post related to Generation Z:\n",
      "[1] What’s a movie that best represents your childhood? There's so many coming of age movies for boomers and millennials, what about for gen-Z? ... (https://reddit.com/r/GenZ/comments/1erotgn/whats_a_movie_that_best_represents_your_childhood/)\n",
      "\n",
      "Here is an excerpt for the post mentioning Millennials:\n",
      "[2] Millenials have less kids, so they have more time and money to spend on themselves. Millenials are also getting married less and later in life, meaning we allocate more effort on self care ... (https://reddit.com/r/AskReddit/comments/1kz5a0n/do_you_think_millennials_generally_look_younger/mv2xf3q/)\n",
      "\n",
      "Here is an excerpt for the post about retro games:\n",
      "[4] Surprised I don’t see project zomboid. Graphics are retro but the game is so well made. Who knew that you needed a pipe wrench (instead of a regular wrench) to plumb the sink! ... (https://reddit.com/r/AskReddit/comments/1iz0bzi/which_game_proves_that_graphics_arent_everything/mezpeha/)\n",
      "\n",
      "Here is an excerpt for the post about future technology:\n",
      "[5] Can't wait for Vision Air 5 launching in 2034 with these transparent displays! ... (https://reddit.com/r/VisionPro/comments/1b36n83/cant_wait_for_vision_air_5_launching_in_2034_with/)\n",
      "\n",
      "However, it's essential to keep in mind that Reddit users can span a wide range of ages and these observations only provide a general sense of the user's interests and familiarity with different generations and technology trends.\n",
      "\n",
      "Not enough data for: [3], [6], and [7] entries.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== BUILD ====\n",
      " Based on the provided Reddit entries, it appears that the user is interested in strategy games, particularly Civilization V (post [2]) and Project Zomboid (post [1]). They seem to enjoy discussing strategies and game mechanics, such as the importance of growth tiles and the use of wallframes instead of regular wrenches (posts [2] and [3]). The user also seems to favor a city-building strategy that involves building cities close to borders or at three-way borders (post [5]). Additionally, they might be an iOS developer working with visionOS (post [6]) and are familiar with ASCII diagrams (post [7]). However, there is not enough data to determine specific details about their build or hardware setup.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== HAIR ====\n",
      " Based on the provided Reddit entries, there is not enough data to accurately describe the user's hair. The only reference to a user's appearance relates to a first date where the user wore a toga, but no information about their hair is given ([4]). The other posts do not mention hair at all.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== SKIN TONE ====\n",
      " Based on the provided Reddit entries, there's not enough data to determine the user's skin tone as the conversations do not contain any statements or references related to physical appearances.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== CLOTHING ====\n",
      " Based on the provided Reddit entries, the users' clothing information is not explicitly mentioned in any of them. Here are a few assumptions that can be made from indirect references or context:\n",
      "\n",
      "1. The user in entry 1 might have worn traditional attire such as a toga for a first date (https://reddit.com/r/AskReddit/comments/1ksghx5/what_happened_on_the_first_date_that_made_you/mtnzw3v/)\n",
      "2. It is unclear what the user in entry 2 was wearing when shopping at Whole Foods (https://reddit.com/r/AskNYC/comments/1hh2tw3/obvious_signs_that_the_rich_has_come_to_take_ur/m2nvkr5/)\n",
      "3. The user in entry 3 mentions visiting various restaurants and cafes, but their clothing is not specified (https://reddit.com/r/FoodNYC/comments/1lc0fio/if_you_had_to_spend_one_full_day_eating_your_way/mxxhvpf/)\n",
      "4. The user in entry 4 discusses personal finance and frugality, but there is no mention of their clothing (https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/)\n",
      "5. The user in entry 5 does not discuss clothing, as they are focused on investing and the stock market (https://reddit.com/r/wallstreetbets/comments/fddw2r/looking_beyond/)\n",
      "6. The user in entry 6 might have been wearing casual or leisurewear while visiting a daytime bathhouse (https://reddit.com/r/AskNYC/comments/1iypf3t/just_laid_off_what_daytime_nyc_stuff_should_i_do/mexk4g8/)\n",
      "7. The user in entry 7 does not mention clothing, but they are discussing game strategy for Manor Lords (https://reddit.com/r/ManorLords/comments/1lq4lst/this_is_acceptable/n1089i9/)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== SETTING ====\n",
      " Based on the provided Reddit entries, it appears that the user's setting could be a mix of urban and digital environments.\n",
      "\n",
      "[1] The user mentions using ASCII diagrams, suggesting they are on a computer or digital device. (https://reddit.com/r/RooCode/comments/1jwf8oq/heres_how_i_make_use_of_the_different_modes_in/mmih39e/)\n",
      "\n",
      "[2] The user mentions visiting a daytime bathhouse, which seems to suggest an urban environment with access to such facilities. (https://reddit.com/r/AskNYC/comments/1iypf3t/just_laid_off_what_daytime_nyc_stuff_should_i_do/mexk4g8/)\n",
      "\n",
      "[3] The user discusses issues with a digital assistant (possibly a voice-enabled one) and its responses degrading over time. (https://reddit.com/r/ChatGPT/comments/1hh2q9r/voice_mode_has_been_brutally_nerfed/m2nxxav/)\n",
      "\n",
      "[4] The user discusses strategy games like Civilization 5, suggesting they are into digital gaming. (https://reddit.com/r/civ5/comments/1h7bmv3/flight_deity/m0lub7c/)\n",
      "\n",
      "[5] The user mentions sending a direct message, which is an online communication method. (https://reddit.com/r/nycrail/comments/1kplupt/anyone_want_this_subway_token_wallet/mt02ziy/)\n",
      "\n",
      "[6] The user lists various food establishments in New York City, implying they are in or have access to this urban location. (https://reddit.com/r/FoodNYC/comments/1lc0fio/if_you_had_to_spend_one_full_day_eating_your_way/mxxhvpf/)\n",
      "\n",
      "[7] The user discusses financial investments, stock options, and gains, suggesting they are involved in the financial market. (https://reddit.com/r/wallstreetbets/comments/fhp5ae/crazy_gains_what_now_should_i_cash_out_or_double/)\n",
      "\n",
      "In summary, the user's setting seems to be a mix of urban and digital environments, with access to computers or smartphones for gaming, messaging, and possibly work. They also appear to have an interest in food and financial markets within their urban location, likely New York City based on the food establishments mentioned.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== POSTURE ====\n",
      " Based on the provided Reddit entries, the user's posture can be described as curious and open-minded. They seem to enjoy exploring various topics such as investment strategies ([1]), urban etiquette ([2]), dating ([3]), art and creativity ([4]), technology ([5], [6]), and spatial computing ([5], [7]). The user appears to be proactive in seeking knowledge, asking questions, and engaging in discussions about diverse subjects.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== OVERALL IMPRESSION ====\n",
      " From the provided Reddit entries, it appears that the user:\n",
      "\n",
      "1. Makes astute observations ([1](https://reddit.com/r/OnePiece/comments/1hok31s/bandage_bandage_no_mi/m4a6r4s/))\n",
      "2. Enjoys pop culture and finds inspiration in it ([2](https://reddit.com/r/VisionPro/comments/1alx270/watching_edgerunners_on_the_moon_feels/))\n",
      "3. Is humble and self-aware, acknowledging their high activity on Reddit ([3](https://reddit.com/r/warriors/comments/1h9zxoq/wife_got_courtside_box_seats_from_her_company_and/m15gny0/))\n",
      "4. Is analytical and forward-thinking ([4](https://reddit.com/r/wallstreetbets/comments/fddw2r/looking_beyond/))\n",
      "5. Has a dry sense of humor and isn't easily swayed by expectations ([5](https://reddit.com/r/AskReddit/comments/1gyerwn/what_gets_you_out_of_bed_in_the_morning/lyr8hvi/))\n",
      "6. Is open to discussing complex immigration-related issues ([6](https://reddit.com/r/AskReddit/comments/1hnx7lj/h1b_holders_do_you_feel_exploited_or_see_it_as_an/))\n",
      "7. Has experienced disappointment, but is resilient ([7](https://reddit.com/r/warriors/comments/1hnvrmw/game_thread_202425_nba_regular_season_golden/m45hfw5/))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== FASHION SENSE ====\n",
      " Based on the provided Reddit entries, the user's fashion sense is not clearly defined as they have discussed a variety of topics unrelated to fashion. However, some indirect inferences can be made:\n",
      "\n",
      "1. The user might appreciate unconventional clothing choices, as shown by their anecdote about wearing a toga on a date ([1]) and their comment regarding photorealistic characters in artwork ([4]).\n",
      "2. There seems to be an emphasis on practicality, as demonstrated in the advice given to pedestrians about not walking slowly and obstructing the way ([5]), and the interest in finding ways to stretch their money ([6]).\n",
      "3. The user shows a potential interest in cartoon-style clothing, as they mention that our eyes are more lenient when it comes to cartoons ([4]).\n",
      "4. The user seems to have an appreciation for observation and detail, as shown by the astute comment about Bandage Parker's bandages in One Piece ([7]).\n",
      "5. The user shows interest in finance and stocks, which might indicate a preference for functional clothing, as they discuss funeral stocks and photorealistic artwork ([2], [3], [4]).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== LIFESTYLE CUES ====\n",
      " Based on the provided Reddit entries, it appears that the user is interested in frugality ([1]), navigating crowded urban spaces considerately ([2]), has an interest or involvement in stock market discussions ([3]), enjoys digital art and photo-realism ([4]), is an iOS developer with a focus on spatial computing ([5]), appreciates informative documentary videos ([6]), and manages resources, such as food, in a strategic manner ([7]). However, it's difficult to determine if this user is a full-time worker, student, or has other specific lifestyle traits without additional information.\n",
      "\n",
      "References:\n",
      "1 - <https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/>\n",
      "2 - <https://reddit.com/r/AskNYC/comments/1h2aw9v/whats_one_thing_you_would_tell_a_tourist_to_never/lzi30jy/>\n",
      "3 - <https://reddit.com/r/wallstreetbets/comments/fddw2r/looking_beyond/>\n",
      "4 - <https://reddit.com/r/ChatGPT/comments/1jpagk5/to_me_the_most_impressive_new_feature_is_the/ml31o85/>\n",
      "5 - <https://reddit.com/r/visionosdev/comments/1b3yugb/best_blogs_tutorial_channels_to_learn/>\n",
      "6 - <https://reddit.com/r/NYCbike/comments/16n85uj/informative_documentary_video_on_youtube/>\n",
      "7 - <https://reddit.com/r/ManorLords/comments/1lihry5/am_i_doing_something_wrong_with_food_management_i/mzc2uit/>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== FRUSTRATIONS ====\n",
      " The users express the following frustrations in their Reddit posts:\n",
      "\n",
      "[1] Frustration with certain city-states (Venice and Austria) buying city states and influencing World Congress votes, as well as Venice's excessive missionary spamming. Link: https://reddit.com/r/civ5/comments/1hhujby/ranking_the_ai_by_how_dangerous_they_are_on_high/m4dc5oo/\n",
      "\n",
      "[2] Frustration with the inconsistencies of referees in the NBA league, and a desire for league review. [The original comment was removed.] Link: https://reddit.com/r/nba/comments/1hcopxo/what_needs_to_happen_for_the_league_to_review_the/\n",
      "\n",
      "[3] Expressing agreement with an observation about a character in One Piece, but it is not clear if this user feels frustrated by anything specific. Link: https://reddit.com/r/OnePiece/comments/1hok31s/bandage_bandage_no_mi/m4a6r4s/\n",
      "\n",
      "[4] Frustration with the Golden State Warriors giving slight hope and then disappointing their fans in NBA basketball games. Link: https://reddit.com/r/warriors/comments/1hnvrmw/game_thread_202425_nba_regular_season_golden/m45hfw5/\n",
      "\n",
      "[5] Sharing enthusiasm for using GCP credits, suggesting a positive experience overall. Link: https://reddit.com/r/warriors/comments/1h9zxoq/wife_got_courtside_box_seats_from_her_company_and/m14y4pr/\n",
      "\n",
      "[6] Frustration with the game mechanics of Civilization 5, specifically mentioning a strategy for managing city states and specialists. Link: https://reddit.com/r/civ5/comments/1h61fr0/on_a_post_about_trade_tariffs/m0a6ok5/\n",
      "\n",
      "[7] Frustration with personal financial management, seeking advice on decision-making processes for purchasing and not buying items. Link: https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== BEHAVIORS ====\n",
      " Based on the provided Reddit entries, here's a summary of the user's behaviors:\n",
      "\n",
      "1. The user seems to be budget-conscious and interested in learning about frugal spending habits ([1](https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/)).\n",
      "2. The user frequents a bar and seems to have a preference for mature, music-oriented environments ([2](https://reddit.com/r/newyorkcity/comments/1lykkqf/i_feel_violated_by_intern_season/)).\n",
      "3. The user appears to be involved in strategy games, specifically Civilization V, and seems to favor rational and specialized strategies ([3](https://reddit.com/r/civ5/comments/1h61fr0/on_a_post_about_trade_tariffs/m0a6ok5/)).\n",
      "4. The user has knowledge about the subway system, specifically regarding conductors and door sensors ([4](https://reddit.com/r/AskNYC/comments/1hcsb8n/what_do_the_conductors_in_the_middle_subway_cars/m1qihqd/)).\n",
      "5. The user seems to advocate for pedestrian etiquette, suggesting not to obstruct the flow of people ([5](https://reddit.com/r/AskNYC/comments/1h2aw9v/whats_one_thing_you_would_tell_a_tourist_to_never/lzi30jy/)).\n",
      "6. The user seems to appreciate and engage in discussions about observational details, as demonstrated by a comment about One Piece ([6](https://reddit.com/r/OnePiece/comments/1hok31s/bandage_bandage_no_mi/m4a6r4s/)).\n",
      "7. The user seems to make pop culture references, as demonstrated by a comment about X ([7](https://reddit.com/r/ChatGPT/comments/1i2pwva/nuclear_hot_take_on_reddit/m7grdiv/)).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== HABITS ====\n",
      " Based on the provided Reddit entries, the user appears to have the following habits:\n",
      "\n",
      "1. The user is interested in living frugally and saving money ([1](https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/)).\n",
      "2. The user mentions Whole Foods, suggesting they might shop there occasionally ([2](https://reddit.com/r/AskNYC/comments/1hh2tw3/obvious_signs_that_the_rich_has_come_to_take_ur/m2nvkr5/)).\n",
      "3. The user seems to appreciate astute observations and details ([3](https://reddit.com/r/OnePiece/comments/1hok31s/bandage_bandage_no_mi/m4a6r4s/)).\n",
      "4. The user enjoys dining out in New York City at various places such as Buvette for breakfast, Lyria for coffee, Tsukemen okiboru for lunch, La cabra for coffee, Isodi for dinner, and Double chicken please for drinks ([4](https://reddit.com/r/FoodNYC/comments/1lc0fio/if_you_had_to_spend_one_full_day_eating_your_way/mxxhvpf/)).\n",
      "5. The user appears to have an interest in Project Zomboid, specifically RVs within the game ([5](https://reddit.com/r/projectzomboid/comments/13dwl0i/3_months_in_the-cleanest-and-most-functional-rv/)).\n",
      "6. The user is an iOS developer and is interested in learning about new featuresets for spacial computing ([6](https://reddit.com/r/visionosdev/comments/1b3yugb/best_blogs_tutorial_channels_to_learn/)).\n",
      "7. The user visits a specific restaurant, Rakus Noodle, once a month and is open to providing feedback on any changes ([7](https://reddit.com/r/FoodNYC/comments/1h87emd/did_rakus_noodle_recipe_change/m0rzprt/)).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== GOALS AND NEEDS ====\n",
      " Based on the provided Reddit entries, here's a description of the user's goals and needs:\n",
      "\n",
      "1. The user seems to have a keen interest in sports, particularly basketball, and is concerned about performance under pressure (performance anxiety). They reference an NBA player named Lindy. [Link](https://reddit.com/r/warriors/comments/1hq7r88/can_someone_explain_why_lindy_is_out_there_over/m4prpxf/)\n",
      "2. The user is frustrated with the inconsistencies of referees in basketball and wants league intervention to address this issue. [Link](https://reddit.com/r/nba/comments/1hcopxo/what_needs_to_happen_for_the_league_to_review_the/)\n",
      "3. The user mentions looking beyond a specific topic, but the context isn't provided, so it's unclear what they're referring to. [Link](https://reddit.com/r/wallstreetbets/comments/fddw2r/looking_beyond/)\n",
      "4. The user shares a detailed itinerary of places to eat and drink in New York City, indicating a love for food and possibly travel or culinary experiences. [Link](https://reddit.com/r/FoodNYC/comments/1lc0fio/if_you_had_to_spend_one_full_day_eating_your_way/)\n",
      "5. The user seeks advice on developing a mental model for making decisions about purchases, showing an interest in personal finance and frugality. [Link](https://reddit.com/r/Frugal/comments/15szxcx/how_do_you_decide_what_to_buy_and_not_buy/)\n",
      "6. The user expresses an interest in spatial tours with 3D maps and 360-degree video, possibly related to virtual reality or map technology. [Link](https://reddit.com/r/VisionPro/comments/1b4yi15/spacial_tours_with_3d_map_and_360_video/)\n",
      "7. The user appears to enjoy strategy games, specifically Civilization V, and is discussing a specific game strategy involving max rationalism, hermit playstyle, internal trade routes, bulb scientists, and space exploration. [Link](https://reddit.com/r/civ5/comments/1h61fr0/on_a_post_about_trade_tariffs/m0a6ok5/)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== MOTIVATION ====\n",
      " Based on the provided Reddit entries, it seems that the user is motivated by three main factors: frugality, self-improvement, and seeking a better life.\n",
      "\n",
      "1. Frugality: The user expresses an interest in stretching their money and finding ways to make purchasing decisions more effectively ([1]). This suggests a motivation towards financial responsibility and saving money.\n",
      "\n",
      "2. Self-improvement: In the context of discussing Civ5, the user seems motivated by strategic thinking and planning, as indicated by their interest in optimizing trade routes and developing mental models for decision-making ([2]). Similarly, in response to a post about basketball, the user expresses disappointment but quickly learns to adapt and move forward, which shows a drive towards self-improvement ([3]).\n",
      "\n",
      "3. Seeking a better life: In discussions related to H1B visas, the user has a personal connection as they have either been an H1B holder or have interacted with people who are. They express gratitude for the opportunity that H1B visas provide in building a better life in the US ([4], [7]). However, it's not clear whether the user feels exploited or not based on the provided data ([4], [6]).\n",
      "\n",
      "Overall, the user appears to be driven by financial responsibility, strategic thinking, and the pursuit of personal growth and a better life.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== PERSONALITY ====\n",
      " Based on the provided Reddit entries, the user appears to be a thoughtful and analytical individual. They demonstrate a strong interest in economics, trade, and financial planning ([1], [4]). The user also shows a keen eye for detail and observational skills, as evidenced by their astute comments about fictional characters ([3], [5]).\n",
      "\n",
      "The user has a sense of humor, as they use emoticons to describe a character from One Piece comically ([5]), and they seem to appreciate the community aspect of Reddit, engaging in conversations and sending direct messages ([6]).\n",
      "\n",
      "Additionally, the user appears to be adaptable and resilient, having moved from one place or situation to another (implied by their immigration status [7]) and demonstrating a hermit-like quality while maintaining internal trade routes online ([1]). However, it is not enough data to ascertain if they are extroverted or introverted.\n",
      "\n",
      "Overall, the user appears to be intelligent, analytical, observant, adaptable, and has a playful side, but further information would be needed for a more comprehensive analysis of their personality.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==== PERSONA QUOTE ====\n",
      " The user's persona seems to be a versatile individual with diverse interests, as suggested by their participation in various subreddits such as r/wallstreetbets, r/OnePiece, and r/AskReddit. They appear to have a sense of humor, as evidenced by their use of emoticons like 🦌 (elephant) in the context of a One Piece character.\n",
      "\n",
      "The user also shows signs of being financially aware, considering their discussion about Robinhood and stocks on r/wallstreetbets. They seem to be thoughtful, as shown by their astute observation about a detail in One Piece.\n",
      "\n",
      "The user seems to have a connection with the anime One Piece, as several of their comments are related to it. However, it's also clear that they enjoy other media, such as movies and games (Project Zomboid). They appear to be engaged in discussions about generational differences, indicating an interest in societal trends and cultural shifts.\n",
      "\n",
      "Overall, the user appears to be a financially aware, humorous, thoughtful individual with diverse interests in various forms of media and sociocultural topics.\n",
      "\n",
      "Quote: \"A versatile, humor-infused, financially aware individual with diverse interests in media and societal trends.\" (Citations: [1], [2], [3], [4], [5], [6], [7])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline\n",
    "persona_output = persona_pipeline(\n",
    "    reddit_user_url,\n",
    "    PERSONA_FIELDS,\n",
    "    REDDIT_CREDENTIALS,\n",
    "    EMBEDDING_MODEL_PATH,\n",
    "    docs_limit=70\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c3c7fb",
   "metadata": {},
   "source": [
    "draaw back was the prompt engineering, the citation was not made for half of the persona fields, so will be trying same above code with different prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 101 posts/comments to reddit_user_data.json\n",
      "✅ Embedding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dba3d58df14ed7b6d8e19443a145e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Generating gender...\n",
      "🔍 Generating age...\n",
      "🔍 Generating build...\n",
      "🔍 Generating hair...\n",
      "🔍 Generating skin tone...\n",
      "🔍 Generating clothing...\n",
      "🔍 Generating setting...\n",
      "🔍 Generating posture...\n",
      "🔍 Generating overall impression...\n",
      "🔍 Generating fashion sense...\n",
      "🔍 Generating lifestyle cues...\n",
      "🔍 Generating frustrations...\n",
      "🔍 Generating behaviors...\n",
      "🔍 Generating habits...\n",
      "🔍 Generating goals and needs...\n",
      "🔍 Generating motivation...\n",
      "🔍 Generating personality...\n",
      "🔍 Generating persona quote...\n",
      "\n",
      "✅ Persona report saved to: persona_report2.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_username(url):\n",
    "    match = re.search(r'reddit\\.com/user/([^/?]+)', url)\n",
    "    return match.group(1) if match else url\n",
    "\n",
    "\n",
    "def fetch_and_save_reddit_user_data(reddit, profile_url, save_path='reddit_user_data.json', limit=100):\n",
    "    username = extract_username(profile_url)\n",
    "    user = reddit.redditor(username)\n",
    "    data = []\n",
    "    for post in user.submissions.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'post',\n",
    "            'title': post.title,\n",
    "            'body': post.selftext,\n",
    "            'permalink': f'https://reddit.com{post.permalink}',\n",
    "            'subreddit': str(post.subreddit)\n",
    "        })\n",
    "    for comment in user.comments.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'comment',\n",
    "            'body': comment.body,\n",
    "            'permalink': f'https://reddit.com{comment.permalink}',\n",
    "            'subreddit': str(comment.subreddit)\n",
    "        })\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\" Saved {len(data)} posts/comments to {save_path}\")\n",
    "\n",
    "\n",
    "def load_reddit_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_faiss_index(docs, embedding_model_path):\n",
    "    model = SentenceTransformer(embedding_model_path)\n",
    "    print(\" Embedding documents...\")\n",
    "    texts = [\n",
    "        f\"Title: {doc['title']}\\nBody: {doc['body']}\" if doc['type'] == 'post' else doc['body']\n",
    "        for doc in docs\n",
    "    ]\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, docs, model\n",
    "\n",
    "\n",
    "def retrieve_docs(query, index, docs, model, k=7):\n",
    "    emb = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(emb, k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "def build_prompt_for_field(field, docs):\n",
    "    prompt = (f\"Based on these numbered Reddit snippets, describe the user's {field}.\"\n",
    "              f\"\\n- Every factual claim must cite the snippet number *and* the link as (Snippet [N], Link: ...).\"\n",
    "              f\"\\n- At the end, write 'Citations:' and list all links you cited (not unreferenced ones).\"\n",
    "              \"\\nExample:\\n\"\n",
    "              '\"The user lives in NYC (Snippet [2], Link: https://reddit.com/r/nyc/comments/xyz123/NYC_life/)\".\\n\\n'\n",
    "              \"Snippets:\\n\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        text = (f\"{doc.get('title', '')} {doc.get('body', '')}\" if doc['type'] == 'post' else doc.get('body', ''))\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        snippet = text[:240]\n",
    "        prompt += f\"[{i}] \\\"{snippet}\\\" (Link: {doc['permalink']})\\n\"\n",
    "    prompt += \"\\nYour answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_ollama_mistral(prompt):\n",
    "    result = subprocess.run(\n",
    "        ['ollama', 'run', 'mistral'],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def persona_pipeline(profile_url, persona_fields, reddit_credentials, embedding_model_path, docs_limit=70, output_path='persona_output.txt'):\n",
    "    # Reddit setup\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_credentials['client_id'],\n",
    "        client_secret=reddit_credentials['client_secret'],\n",
    "        user_agent=reddit_credentials['user_agent']\n",
    "    )\n",
    "    \n",
    "    # Step 1: Fetch and save posts/comments\n",
    "    fetch_and_save_reddit_user_data(reddit, profile_url, 'reddit_user_data.json', limit=docs_limit)\n",
    "    \n",
    "    # Step 2: Load data and build FAISS index\n",
    "    docs = load_reddit_data('reddit_user_data.json')\n",
    "    index, meta, embed_model = build_faiss_index(docs, embedding_model_path)\n",
    "    \n",
    "    # Step 3: Generate persona field-by-field\n",
    "    results = {}\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(f\" Persona Report for Reddit User: {extract_username(profile_url)}\\n\\n\")\n",
    "        for field in persona_fields:\n",
    "            print(f\"🔍 Generating {field}...\")\n",
    "            context = retrieve_docs(field, index, meta, embed_model, k=7)\n",
    "            prompt = build_prompt_for_field(field, context)\n",
    "            answer = run_ollama_mistral(prompt)\n",
    "            results[field] = answer.strip()\n",
    "            f_out.write(f\"\\n=== {field.upper()} ===\\n{answer.strip()}\\n\")\n",
    "    \n",
    "    print(f\"\\n Persona report saved to: {output_path}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    reddit_user_url = \"https://www.reddit.com/user/kojied/\"  \n",
    "\n",
    "    persona_output = persona_pipeline(\n",
    "        profile_url=reddit_user_url,\n",
    "        persona_fields=PERSONA_FIELDS,\n",
    "        reddit_credentials=REDDIT_CREDENTIALS,\n",
    "        embedding_model_path=EMBEDDING_MODEL_PATH,\n",
    "        docs_limit=70,\n",
    "        \n",
    "        output_path='persona_report2.txt'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e6a84",
   "metadata": {},
   "source": [
    "now the code is working well, below is the full code in single cell , you can use it for your demo purposes. I will be using it for 2nd user you have provided me with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea1207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved 15 posts/comments to reddit_user_data.json\n",
      " Embedding documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fb6e754f3844b3991fa49bb9de3f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generating gender...\n",
      " Generating age...\n",
      " Generating build...\n",
      " Generating hair...\n",
      " Generating skin tone...\n",
      " Generating clothing...\n",
      " Generating setting...\n",
      " Generating posture...\n",
      " Generating overall impression...\n",
      " Generating fashion sense...\n",
      " Generating lifestyle cues...\n",
      " Generating frustrations...\n",
      " Generating behaviors...\n",
      " Generating habits...\n",
      " Generating goals and needs...\n",
      " Generating motivation...\n",
      " Generating personality...\n",
      " Generating persona quote...\n",
      "\n",
      "Persona report saved to: persona_report-2.txt\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Your Reddit API credentials\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"Ze0l8Tb5iILjJLTDxKRflA\",\n",
    "    \"client_secret\": \"PcqO8CtAQO7pIoGuotuNeb_qPWrAMQ\",\n",
    "    \"user_agent\": \"my_reddit_scraper\"\n",
    "}\n",
    "\n",
    "# Path to your local SentenceTransformer model folder\n",
    "EMBEDDING_MODEL_PATH = r\"C:\\amrita_uni\\Projects\\BeyondChats\\model\"\n",
    "\n",
    "# Persona fields to extract\n",
    "PERSONA_FIELDS = [\n",
    "    \"gender\", \"age\", \"build\", \"hair\", \"skin tone\", \"clothing\", \"setting\", \"posture\",\n",
    "    \"overall impression\", \"fashion sense\", \"lifestyle cues\", \"frustrations\", \"behaviors\",\n",
    "    \"habits\", \"goals and needs\", \"motivation\", \"personality\", \"persona quote\"\n",
    "]\n",
    "\n",
    "def extract_username(url):\n",
    "    match = re.search(r'reddit\\.com/user/([^/?]+)', url)\n",
    "    return match.group(1) if match else url\n",
    "\n",
    "\n",
    "def fetch_and_save_reddit_user_data(reddit, profile_url, save_path='reddit_user_data.json', limit=100):\n",
    "    username = extract_username(profile_url)\n",
    "    user = reddit.redditor(username)\n",
    "    data = []\n",
    "    for post in user.submissions.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'post',\n",
    "            'title': post.title,\n",
    "            'body': post.selftext,\n",
    "            'permalink': f'https://reddit.com{post.permalink}',\n",
    "            'subreddit': str(post.subreddit)\n",
    "        })\n",
    "    for comment in user.comments.new(limit=limit):\n",
    "        data.append({\n",
    "            'type': 'comment',\n",
    "            'body': comment.body,\n",
    "            'permalink': f'https://reddit.com{comment.permalink}',\n",
    "            'subreddit': str(comment.subreddit)\n",
    "        })\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\" Saved {len(data)} posts/comments to {save_path}\")\n",
    "\n",
    "\n",
    "def load_reddit_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_faiss_index(docs, embedding_model_path):\n",
    "    model = SentenceTransformer(embedding_model_path)\n",
    "    print(\" Embedding documents...\")\n",
    "    texts = [\n",
    "        f\"Title: {doc['title']}\\nBody: {doc['body']}\" if doc['type'] == 'post' else doc['body']\n",
    "        for doc in docs\n",
    "    ]\n",
    "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, docs, model\n",
    "\n",
    "\n",
    "def retrieve_docs(query, index, docs, model, k=7):\n",
    "    emb = model.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(emb, k)\n",
    "    return [docs[i] for i in I[0]]\n",
    "\n",
    "def build_prompt_for_field(field, docs):\n",
    "    prompt = (f\"Based on these numbered Reddit snippets, describe the user's {field}.\"\n",
    "              f\"\\n- Every factual claim must cite the snippet number *and* the link as (Snippet [N], Link: ...).\"\n",
    "              f\"\\n- At the end, write 'Citations:' and list all links you cited (not unreferenced ones).\"\n",
    "              \"\\nExample:\\n\"\n",
    "              '\"The user lives in NYC (Snippet [2], Link: https://reddit.com/r/nyc/comments/xyz123/NYC_life/)\".\\n\\n'\n",
    "              \"Snippets:\\n\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        text = (f\"{doc.get('title', '')} {doc.get('body', '')}\" if doc['type'] == 'post' else doc.get('body', ''))\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        snippet = text[:240]\n",
    "        prompt += f\"[{i}] \\\"{snippet}\\\" (Link: {doc['permalink']})\\n\"\n",
    "    prompt += \"\\nYour answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_ollama_mistral(prompt):\n",
    "    result = subprocess.run(\n",
    "        ['ollama', 'run', 'mistral'],\n",
    "        input=prompt,\n",
    "        text=True,\n",
    "        capture_output=True,\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "def persona_pipeline(profile_url, persona_fields, reddit_credentials, embedding_model_path, docs_limit=70, output_path='persona_output.txt'):\n",
    "    # Reddit setup\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=reddit_credentials['client_id'],\n",
    "        client_secret=reddit_credentials['client_secret'],\n",
    "        user_agent=reddit_credentials['user_agent']\n",
    "    )\n",
    "    \n",
    "    # Step 1: Fetch and save posts/comments\n",
    "    fetch_and_save_reddit_user_data(reddit, profile_url, 'reddit_user_data.json', limit=docs_limit)\n",
    "    \n",
    "    # Step 2: Load data and build FAISS index\n",
    "    docs = load_reddit_data('reddit_user_data.json')\n",
    "    index, meta, embed_model = build_faiss_index(docs, embedding_model_path)\n",
    "    \n",
    "    # Step 3: Generate persona field-by-field\n",
    "    results = {}\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        f_out.write(f\" Persona Report for Reddit User: {extract_username(profile_url)}\\n\\n\")\n",
    "        for field in persona_fields:\n",
    "            print(f\" Generating {field}...\")\n",
    "            context = retrieve_docs(field, index, meta, embed_model, k=7)\n",
    "            prompt = build_prompt_for_field(field, context)\n",
    "            answer = run_ollama_mistral(prompt)\n",
    "            results[field] = answer.strip()\n",
    "            f_out.write(f\"\\n=== {field.upper()} ===\\n{answer.strip()}\\n\")\n",
    "    \n",
    "    print(f\"\\nPersona report saved to: {output_path}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    reddit_user_url = \"https://www.reddit.com/user/Hungry-Move-6603/\"  \n",
    "\n",
    "    persona_output = persona_pipeline(\n",
    "        profile_url=reddit_user_url,\n",
    "        persona_fields=PERSONA_FIELDS,\n",
    "        reddit_credentials=REDDIT_CREDENTIALS,\n",
    "        embedding_model_path=EMBEDDING_MODEL_PATH,\n",
    "        docs_limit=70,\n",
    "        \n",
    "        output_path='persona_report-2.txt'\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4962e9",
   "metadata": {},
   "source": [
    "since there are few posts it can refer to , it wasnt a good way to build persona based on this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
